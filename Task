from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.firefox_profile import FirefoxProfile
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait # подгрузка метода ожидания загрузки страницы
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains # реализация прокрутки страницы к необходимому элементу

import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, time, timedelta
import time
import re
import json
from pprint import pprint


from selenium import webdriver
import time

def generate_url(base_url, num):
    """Функция для генерация url страниц"""
    print('URL:', current_url, sep='\n')
    print()
    find_key_word = base_url.find('q=')
    url = f'https://www.foroffice.ru/products/search/?' + current_url[find_key_word:]+f'&PAGEN_1={num}'
    # f'&PAGEN_1={num}' - эта часть url отвечает за номер страницы в результате поиска
    return url


def info_from_page(current_url, num):
    """Функция для обработкы содержимого страницы результатов поиска
    return: словарь с данными о результате поиска
    """
    # Так как ссылка на станицу от Selenium отличается от ссылки которую получаю при ручном поиске, и ссылка от Seleium не парсится, но эта ссылка содержит кодировку котурую можно применить для создания ссылки которую можно пропарсить
    url = generate_url(current_url, num)
    print(url)
    fake_user = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0"
    response = requests.get(url, headers={'User-Agent': fake_user}, verify=False)  # Получаю get запрос по API
    print(response, url)
    # , verify=False
    # Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
    soup = BeautifulSoup(response.content, 'html.parser')
    release_links = []
    result_dict = {}  # Словарь с информацией о товаре

    # Собираю информацию о товаре с первой странице
    # print(soup.find_all('div', ('class', 'right col')))
    for link, info in zip(soup.find_all('div', ('class', "center col")), soup.find_all('div', ('class', 'right col'))):
        try:
            name_item = str(link.find('a').text.strip())  # наименование товара
            print(name_item)

            find_link = 'https://www.foroffice.ru/' + str(link.find('a').get('href'))  # ссылка
            print(find_link)

            price = info.find('span').text.strip().split("\n")  # вывожу стоимость товара и валюту в список
            print(price)

            result_dict[find_link] = [name_item, price]
        except:
            release_links.append('None')
    time.sleep(1)
    return result_dict

search = input("Поиск: ")

# Настройки параметров ссесии браузера
option = webdriver.ChromeOptions()
# option = webdriver.FirefoxOptions()

option.add_argument("start-maximized")

# создаю окно браузера и передаю ему параметры
browser = webdriver.Chrome(options=option)
# browser = webdriver.Firefox(options=option)

browser.get('https://www.foroffice.ru/') # вызываю страницу сайта промышленного оборудования в браузере


# wait = WebDriverWait(browser, 30)
# Ввожу информацию в строку поиска и кликаю на "лупу" для осуществления поиска по сайту
input_web = browser.find_element(By.ID, "__BVID__17")
input_web.send_keys(search)
element = WebDriverWait(browser, 20).until(EC.element_to_be_clickable((By.XPATH, '//button[@class="btn btn-blue search-button"]')))
element.click()
time.sleep(2)

num_page = 3
result_dict = dict()
for i in range(1, num_page):
    # Получаю url текущей страница
    current_url = str(browser.current_url).strip()
    result_dict = {**result_dict, **info_from_page(current_url, i)}
    time.sleep(3)


pprint(result_dict)


input()
browser.quit()
