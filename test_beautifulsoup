import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime, time, timedelta
import time
import re
import json
from fake_useragent import UserAgent
from lxml import html
from lxml import etree



ua = UserAgent()


fake_user = ua.random # создаю фейкового юзера

# Запрос веб-страницы
url = f'https://www.foroffice.ru/search/search_results.php?q=%F0%E5%E7%E0%EA'
response = requests.get(url, verify=False) # Получаю get запрос по API
print(response)

# dom = html.fromstring(response.text)
# print(dom)
# Парсинг HTML-содержимого веб-страницы с помощью Beautiful Soup
soup = BeautifulSoup(response.text, 'html.parser')
print(soup)
#
# Создаю списко ссылок вакансий
release_links = []
print(soup.find_all("div", attrs={"id": "scripts-from-app"}))
for link in soup.find_all('div', ('class', 'name-block')):
    try:
        release_links.append(link.find('a').get('href'))
    except:
        release_links.append('None')

print(release_links)
